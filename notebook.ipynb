{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ec8d6a-4218-49b9-a549-0977cec82967",
   "metadata": {
    "id": "MVpsYfWg3z0B"
   },
   "source": [
    "# PW1 - Handwritten character recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d682452d-e56e-4de8-a420-4418d63790a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your name here (e.g. \"Edmond Dantès\") so I can grade your work\n",
    "your_name = \"Anne-Laure Tanoh\"\n",
    "assert your_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8538cf7-81e0-481a-8059-be38b611aafb",
   "metadata": {
    "id": "8CcAqNjJ3z0F"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math, sys, os, torch, torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c864b86e-6e6b-4d8a-82c8-6822072a676f",
   "metadata": {
    "id": "3Wxb9pdV3z0F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: False \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa038da6-b812-4053-9b72-8ff97ceb3b9b",
   "metadata": {
    "id": "1Sjq8zzf3z0G"
   },
   "source": [
    "We will be training many models. Select a number of epochs to train each model. If you are using a slow machine, or if you want to restart training often and have many development iterations, we suggest `NUM_EPOCH = 2`. If you are using a fast machine, or have a GPU available, of if you are confident that you can write accurate code first try, you will get better accuracies by increasing this constant. You could be able to afford up to `NUM_EPOCH = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "137adce6-13cc-4ccc-aaf3-42230e322a9b",
   "metadata": {
    "id": "L9CF0H4O3z0G"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce850e9-ef4e-4323-af2d-be6b94e98994",
   "metadata": {
    "id": "65e20f5e"
   },
   "source": [
    "# Part A - Linear, MLP, and CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aff4bd-8b1d-4531-89d8-64ce4e2b6357",
   "metadata": {
    "id": "KSAiV2ov3z0H"
   },
   "source": [
    "## Handwritten digit recognition dataset\n",
    "\n",
    "We will use the MNIST database (Modified National Institute of Standards and Technology database). It contains tens of thousands of pictures of handwritten digits. This database was compiled in 1994, as part of the effort in the 1990s to standardize automation of sorting devices with human input, for instance sorting mail with handwritten postal codes at the post office. This is now often considered one of the first real successes of neural networks, and the first easy example on which performance of new such algorithms is tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429898a-0f57-4cdf-9aa6-7b1121ee4e53",
   "metadata": {},
   "source": [
    "Load the dataset (train and test splits) using `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e1c523-4b78-492a-b103-2861af8c3d89",
   "metadata": {
    "id": "Zu3hU4dQ3z0H"
   },
   "outputs": [],
   "source": [
    "root_dir = './data/MNIST/'\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "train_minist=torchvision.datasets.MNIST(\n",
    "    root=root_dir,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    "    )\n",
    "test_minist=torchvision.datasets.MNIST(\n",
    "    root=root_dir,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fbaba-b01f-46d0-a1d7-47c59faaf042",
   "metadata": {},
   "source": [
    "How many examples in each split? \n",
    "\n",
    "Plot the first image and label of the training set using `matplotlib`\n",
    "\n",
    "What is the input dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083f07f4-e129-4e9b-b26e-a32fd4bdb99d",
   "metadata": {
    "id": "9fgMls5P3z0I",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd1a28dd90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "print(\"Label is:\",train_minist.targets[0].item())\n",
    "plt.imshow(train_minist.data[0], cmap='gray', interpolation='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2fbd26-b017-435a-822e-ad382eabd8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "print(\"Input dimension:\",train_minist.data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6380c03-1427-4dea-974e-5a38621ea6c1",
   "metadata": {
    "id": "Guv5_hY63z0L"
   },
   "source": [
    "# A.1 - Linear features\n",
    "\n",
    "We start with a very simple model, linear with respect to pixel values.\n",
    "Use a `preprocess` function to downsample the image to 7x7 pixels, then flatten it and use a `torch.nn.Linear` model.\n",
    "\n",
    "The torch average-pooling function is `torch.nn.functional.avg_pool2d`, check the documentation to set the arguments properly.\n",
    "DO NOT use your implementation of average-pooling, it would take prohibitively long to train and you would not finish the practical.\n",
    "If the training takes too long, go back to the first section and lower the `NUM_EPOCH` constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb8e5d-7d84-4a59-aa5d-e942ffa22aa8",
   "metadata": {},
   "source": [
    "Again, use matplotlib to visualize an example of downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac111d5c-dfa2-44e2-9a57-f2f4c2caff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efcd4445310>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "def preprocess(image):\n",
    "    resized_image=torch.nn.functional.avg_pool2d(image, kernel_size=4)\n",
    "    resized_image=resized_image.view(-1)\n",
    "    return resized_image\n",
    "image, label=train_minist[0]\n",
    "print(\"Label is:\",label)\n",
    "plt.imshow(preprocess(image).view(7, 7).detach(), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa4adcb-be17-4f0d-ba9f-4da062227230",
   "metadata": {
    "id": "2v3GqEPU3z0L"
   },
   "outputs": [],
   "source": [
    "### YOUR ( MODEL / PREPROCESSING ) CODE HERE ###\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(49, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5becae4-e27f-43ed-a1c5-ced8c16f4cac",
   "metadata": {
    "id": "qeWZ7DeNMG20"
   },
   "source": [
    "## A.2 - Loss and optimizer\n",
    "Create a cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85b789e",
   "metadata": {
    "id": "a85b789e"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model=LinearModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aec31b-3d2b-4b29-ad85-e88a6e25660a",
   "metadata": {
    "id": "ZCnlsh9iMhx_"
   },
   "source": [
    "## A.3 - Training and testing loops\n",
    "Finally, create the functions `train(model, epoch, preprocess, optimizer)` and `test(model)` to train (one epoch with SGD and a learning rate of $10^{-3}$) and test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acac0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c065ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000  examples in train split\n",
      "10000  examples in test split\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "trainloader = torch.utils.data.DataLoader(train_minist, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_minist, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_minist),\" examples in train split\")\n",
    "print(len(test_minist),\" examples in test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9749c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, preprocess, optimizer):\n",
    "    model.to(device).train()\n",
    "    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        accuracy_metric.reset()\n",
    "        for inputs, labels in trainloader:\n",
    "            # preprocess + move to device\n",
    "            images = torch.stack([preprocess(img) for img in inputs]).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward / backward / step\n",
    "            optimizer.zero_grad()\n",
    "            outputs     = model(images)\n",
    "            loss_value  = loss(outputs, labels)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update accuracy metric\n",
    "            accuracy_metric.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "        # accumulate per‐epoch accuracy\n",
    "        epoch_acc = accuracy_metric.compute().item()\n",
    "        total_acc += epoch_acc\n",
    "\n",
    "    train_accuracy = total_acc / epoch\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2%}\")\n",
    "    return optimizer, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3484a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, preprocess):\n",
    "    model.to(device).eval()\n",
    "    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "    preds, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            images = torch.stack([preprocess(img) for img in inputs]).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds.append(outputs.argmax(dim=1))\n",
    "            targets.append(labels)\n",
    "\n",
    "    preds   = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    accuracy_metric.update(preds, targets)\n",
    "    test_accuracy = accuracy_metric.compute().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa359382-2580-44ae-bcdb-3f18cf1f4c61",
   "metadata": {
    "id": "_t4SiXk33z0L"
   },
   "source": [
    "You should get at least 85\\% test accuracy even with only 2 epochs. We will be aiming for around 95\\% test accuracy and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f12bf85-45c6-41d9-87f3-0601bc4b6339",
   "metadata": {
    "id": "nBmfvtl6UbUe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 74.65%\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "optimizer, train_acc = train(model, NUM_EPOCH, preprocess, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3baedd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.55%\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28bb4e-7178-4340-9247-f69591e86dcf",
   "metadata": {
    "id": "RrwYAMMBEUPN"
   },
   "source": [
    "## A.4 - Multi-layer perceptron (MLP)\n",
    "\n",
    "Create a class MLP that creates an MLP of given width and depth, and use it to create a 3-layer MLP of width $100$. We will assume that `width > 0` and `depth > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79cb6479-d475-4f0e-b0f7-fdd107d9e835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8353cd9",
    "outputId": "c7f5eeaf-0638-45c3-842e-4372d21ff712"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,depth,width):\n",
    "        super().__init__()\n",
    "        if width < 0 or depth < 0:\n",
    "            raise ValueError(\"Width and depth must be non-negative.\") \n",
    "        self.width=width\n",
    "        self.depth=depth\n",
    "        layers=[]\n",
    "\n",
    "        if self.depth == 0:\n",
    "            layers.append(nn.Linear(49, 10))\n",
    "        else:\n",
    "            # First hidden layer\n",
    "            layers.append(nn.Linear(49, self.width))\n",
    "            layers.append(nn.ReLU())\n",
    "            for num_layer  in range(1,self.depth):\n",
    "                layers.append(nn.Linear(self.width, self.width))\n",
    "                layers.append(nn.ReLU())\n",
    "            # Output layer\n",
    "            layers.append(nn.Linear(self.width, 10))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1841a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 33.39%\n"
     ]
    }
   ],
   "source": [
    "model=MLP(3,100)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer, train_acc = train(model, NUM_EPOCH, preprocess, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25f30c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.32%\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca5a11-93e4-47ab-81f1-e4ee3cae45c9",
   "metadata": {
    "id": "v1czyC9R3z0R"
   },
   "source": [
    "# A.5 - Deep convolutional model\n",
    "\n",
    "Write a convolutional model, with learned features.\n",
    "Use two layers, one convolutional with 8 filters of size 3x3, then take a relu and max-pool with kernel size 2, and finally flatten and add a Linear layer. You can use the identity as pre-processing function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933e318-5521-4bf8-b011-bdb95ed0d8b0",
   "metadata": {},
   "source": [
    "\n",
    "Here is a little animation to remind you of the sliding window principle of convolutions.\n",
    "\n",
    "![conv](https://github.com//vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44c87b30-36d7-4567-b009-a41413afac5b",
   "metadata": {
    "id": "l4QOi_oe3z0R"
   },
   "outputs": [],
   "source": [
    "class ConvModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(13 * 13 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b8e1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 88.58%\n"
     ]
    }
   ],
   "source": [
    "model=ConvModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "def preprocess2(image):\n",
    "    return image\n",
    "optimizer, train_acc = train(model, NUM_EPOCH, preprocess2, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2087fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.03%\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model, preprocess2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0313eed-03ad-461d-9ddb-bb690c631149",
   "metadata": {
    "id": "pbQ1LUqf3z0R"
   },
   "source": [
    "You should be able to get around 97\\% to 98\\% accuracy with this model. Try increasing the NUM_EPOCH constant and watch what happens to test accuracy and train accuracy as training progresses further.\n",
    "\n",
    "Write a deeper convolutional model, with one convolutional layer as previously, but three linear layers with relu activations after that.\n",
    "Use `h = 100` hidden neurons. How does the test accuracy compare with the previous two-layer network ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792791e1-0514-4749-a3fb-d27eb10a1bd2",
   "metadata": {
    "id": "YM-OhC123z0R"
   },
   "outputs": [],
   "source": [
    "class ConvDeepModel(torch.nn.Module):\n",
    "    def __init__(self, h=100):\n",
    "        super(ConvDeepModel, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd763b-b36a-461c-ad34-f472e62ce46d",
   "metadata": {},
   "source": [
    "## A.6 Visualisations of convolutions\n",
    "\n",
    "After training your model, let's see what features it has learned!\n",
    "\n",
    "Plot an image from the test set then plot all 8 feature maps extracted by the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e5284c2-4210-4ccc-bf05-5e722bdabf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc457ad1-3240-4ffb-a8f3-98c6554dfce5",
   "metadata": {
    "id": "riu_K1at3z0R"
   },
   "source": [
    "# Part B - Residual models\n",
    "\n",
    "## B.1 - Residual blocks\n",
    "\n",
    "Write a residual block with two linear layers to learn a function $\\mathbb{R}^d \\to \\mathbb{R}^d$ with $h < d$ hidden neurons.\n",
    "Write a convolutional residual block with the same idea. What hyperparameter acts as the number of hidden neurons in convolutional blocks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aec298d4-4372-4a1f-be53-a278d051c0f6",
   "metadata": {
    "id": "gkCILVwd3z0R"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, d, h):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0097d-269c-40f4-b182-bdc79bd5c512",
   "metadata": {
    "id": "huKWOvVc3z0S"
   },
   "source": [
    "## B.2 - Stacking residual blocks\n",
    "\n",
    "Use a single convolution layer, followed by a relu and max-pool, then an arbitrary number of residual blocks as defined above, and finish with a linear layer. Can you match the accuracy of the two-layer network ? Can you exceed it ? What happens when you increase the number of layers ? Look at the details of the ResNet architecture on the lecture's slides to get an idea of how to increase the number of hidden neurons and the number of layers. One of the strengths of ResNets was there relatively low number of parameters compared\n",
    "to a multi-layer architecture like that of the previous section, does this show in your experiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf0d4de4-11e8-490c-8262-064b3e841fa5",
   "metadata": {
    "id": "U4gg9uWg3z0S"
   },
   "outputs": [],
   "source": [
    "class ResidualModel(torch.nn.Module):\n",
    "    def __init__(self, l, h, k=3, out=8):\n",
    "        super(ResidualModel, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df418703-b1e1-4276-8ba7-dd5622c7b7cd",
   "metadata": {
    "id": "2nes_ZtBoBu0"
   },
   "source": [
    "# Part C - Reimplementing loss functions\n",
    "\n",
    "## C.0 - Combining losses\n",
    "First, we recall that, for a batch of score vectors $s\\in\\mathbb{R}^{n\\times C}$ and true labels $y\\in[1,C]^n$, **cross entropy** is defined as\n",
    "$$CE(s, y) = -\\frac{1}{n}\\sum_{i=1}^n \\log\\left( \\mbox{softmax}(s_i)_{y_i} \\right)$$\n",
    "\n",
    "where $\\mbox{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$ is the probability associated to class $i\\in[1,C]$ for a score vector $x\\in\\mathbb{R}^C$.\n",
    "\n",
    "Let's try to compute cross-entropy in three different ways (see the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)):\n",
    "1. Using `nn.CrossEntropyLoss()`.\n",
    "2. Using `nn.NLLLoss()` and `nn.LogSoftmax()`.\n",
    "3. Using `nn.NLLLoss()` and `nn.Softmax()`.\n",
    "\n",
    "Check that the output is the same for all three methods on Gaussian random scores `torch.randn(n_batch, n_classes)` and random labels `torch.randint(0, n_classes, [n_batch])`, where `n_batch=4` and `n_classes=10`. Note that the scores are real valued vectors while the labels are integers corresponding to the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b6c172",
   "metadata": {
    "id": "e1b6c172"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba900c6-f7b2-4179-a2f1-f1eede4b6697",
   "metadata": {
    "id": "TWKaTBVd5ftN"
   },
   "source": [
    "## C.1 - Re-implementation\n",
    "Now re-implement cross-entropy using base functions (`torch.log`, `torch.exp`, `torch.sum`, etc...). Verify that your function returns the same value as Pytorch's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "EfA-3-E7qwgF",
   "metadata": {
    "id": "EfA-3-E7qwgF"
   },
   "outputs": [],
   "source": [
    "def ce(logits, targets):\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ec13a-330a-4073-99b9-2eb9dd42d1f8",
   "metadata": {
    "id": "OFG0QfKN7WtO"
   },
   "source": [
    "## C.2 - Stability analysis\n",
    "Softmax probabilities can be relatively unstable due to their use of exponentials. Pytorch implementations thus usually use log probas or logits to avoid overflows or floating point errors. Test all methods (including your own) on Gaussian random scores of standard deviation equal to $100$. Which methods are stable? Why? Is it an issue in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "741771f5-864d-446a-b654-3b4f5a2598ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecaaf7c-b21f-4e18-ac53-1a77adbc60bf",
   "metadata": {
    "id": "Y3y4BfwbBIGy"
   },
   "source": [
    "Re-implement a stable version of cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83d00dab-a37e-48da-880f-81f10efdc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_ce(logits, targets):\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
